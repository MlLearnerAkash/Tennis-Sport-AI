{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/charlie/ATPIL/CropVideo\n",
      "/home/charlie/ATPIL\n",
      "/home/charlie/ATPIL/CropVideo\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# In a shell, first run:\n",
    "# conda env create -f yolo_env.yml\n",
    "# then interact with this notebook within that environment (e.g select interpreter in VScode)\n",
    "!pwd\n",
    "\n",
    "# Do you have the data? No, then gdown it\n",
    "%cd ..\n",
    "if not os.path.exists('data'):\n",
    "    !gdown 'https://drive.google.com/uc?id=1wH7yAYCNVi-64YKsnw9M-0GwkiqkDZaZ'\n",
    "    !unzip data.zip\n",
    "    !rm data.zip\n",
    "%cd CropVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Cropping\n",
    "This notebook demonstrates how videos are cropped to include only the player bounding boxes for further processing.\n",
    "For this purpose, we use the [**YOLOv8**](https://ultralytics.com) object detection model to detect the players in the video frames. The 'maximum' bounding box coordinates of the detected players are used to crop the video frames. The cropped video frames are then stitched together to form the final video for each clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8s.pt to 'yolov8s.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21.5M/21.5M [00:03<00:00, 7.19MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/full_clips/forehand.mp4\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['lapx>=0.5.2'] not found, attempting AutoUpdate...\n",
      "Collecting lapx>=0.5.2\n",
      "  Downloading lapx-0.5.9-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /home/charlie/miniconda3/envs/yolo/lib/python3.9/site-packages (from lapx>=0.5.2) (1.26.4)\n",
      "Downloading lapx-0.5.9-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lapx\n",
      "Successfully installed lapx-0.5.9\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 3.3s, installed 1 package: ['lapx>=0.5.2']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "Cropped Video Dims:  274 307\n",
      "Finished forehand.mp4\n",
      "../data/full_clips/run_backhand.mp4\n",
      "Cropped Video Dims:  226 292\n",
      "Finished run_backhand.mp4\n",
      "../data/full_clips/return.mp4\n",
      "Cropped Video Dims:  190 308\n",
      "Finished return.mp4\n",
      "../data/full_clips/front_backhand.mp4\n",
      "Cropped Video Dims:  96 169\n",
      "Finished front_backhand.mp4\n",
      "../data/full_clips/stand_backhand.mp4\n",
      "Cropped Video Dims:  196 320\n",
      "Finished stand_backhand.mp4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_dir = '../data/full_clips'\n",
    "\n",
    "PERSON_CLASS = 0\n",
    "PADDING = 0.05\n",
    "\n",
    "# Run on m1 mac \n",
    "for v in os.listdir(data_dir):\n",
    "    video = os.path.join(data_dir, v)\n",
    "    # reset model trackers\n",
    "    model = YOLO('yolov8s.pt')\n",
    "    print(video)\n",
    "\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    video_width, video_height, fps = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)), int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    ret = True\n",
    "    # the player we want will be the human who spends the most time in the bottom middle half of the frame\n",
    "    #NOTE: This is a very simple heuristic, and WILL PROBABLY NOT WORK for anything other than broadcast video\n",
    "    positions = {}\n",
    "    frameId = 0\n",
    "    while ret:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        results = model.track(frame, persist=True, verbose=False )\n",
    "        for r in results:\n",
    "            for b in r.boxes:\n",
    "                # each b is a detection\n",
    "                if b.cls == PERSON_CLASS:\n",
    "                    id = int(b.id[0])            # Tracking ID of the person\n",
    "                    # Store the position of the person in the frame (xyxyn format)\n",
    "                    if id not in positions:\n",
    "                        positions[id] = np.zeros((num_frames, 4)) # x1, y1, x2, y2\n",
    "                    positions[id][frameId] = b.xyxyn\n",
    "        frameId += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    average_positions = {}\n",
    "    for k in positions:\n",
    "        # get average (where not all zeros)\n",
    "        average_positions[k] = np.mean(positions[k][np.sum(positions[k], axis=1) != 0], axis=0)\n",
    "        average_positions[k] = ((average_positions[k][0] + average_positions[k][2]) / 2, (average_positions[k][1] + average_positions[k][3]) / 2)\n",
    "    # likely candidate will be closest to (0.5, 0.75)\n",
    "    target_x, target_y = 0.5, 0.75\n",
    "    if 'front' in v:\n",
    "        target_y = 0.25 # grab top player if front in video\n",
    "    # sort parallel arrays:\n",
    "    ids = list(average_positions.keys())\n",
    "    av_positions = list(average_positions.values())\n",
    "    ids, av_positions = zip(*sorted(zip(ids, av_positions), key=lambda x: (x[1][0] - target_x)**2 + (x[1][1] - target_y)**2))\n",
    "\n",
    "    best_id = 0\n",
    "    person = positions[ids[best_id]]      # bounding box in each frame for the person we want\n",
    "\n",
    "    # we need at least 90% of the frames to have a bounding box for the person or we skip the video\n",
    "    while np.sum(np.sum(person, axis=1) != 0) < 0.9 * num_frames:\n",
    "        num_frames_appear = np.sum(np.sum(person, axis=1) != 0)\n",
    "        print(f\"person {int(ids[best_id])} does not appear in enough frames ({num_frames_appear} / {num_frames}), trying next person...\")\n",
    "\n",
    "        if best_id + 1 < len(ids):\n",
    "            print(\"No more people in the video\")\n",
    "            break\n",
    "        best_id += 1\n",
    "        person = positions[ids[best_id]]\n",
    "\n",
    "    # # find maximum height and width of the bounding box\n",
    "    bbox_max_h = 0\n",
    "    bbox_max_w = 0\n",
    "    for x1, y1, x2, y2 in person:\n",
    "        bbox_max_h = max(bbox_max_h, (y2 - y1) * video_height)  # Have to multiply by width because the bounding box is normalized\n",
    "        bbox_max_w = max(bbox_max_w, (x2 - x1) * video_width)   # Have to multiply by height because the bounding box is normalized\n",
    "\n",
    "    bbox_max_h = int(bbox_max_h * (1 + PADDING))\n",
    "    bbox_max_w = int(bbox_max_w * (1 + PADDING))\n",
    "    # for each frame in the video, crop to a max_height x max_width box centered on the person\n",
    "    # and save to a new video in '../data/cropped_clips'\n",
    "    # also save a json file with the actual bounding box for each frame of the video\n",
    "    # in the same directory\n",
    "    out_dir = '../data/cropped_clips'\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_video = os.path.join(out_dir, v)\n",
    "    out_json = out_video.replace('.mp4', '.json')\n",
    "\n",
    "    print(\"Cropped Video Dims: \", bbox_max_w, bbox_max_h)\n",
    "    # Video writer for avi\n",
    "    out_cap = cv2.VideoWriter(out_video, cv2.VideoWriter_fourcc(*'mp4v'), fps, (bbox_max_w, bbox_max_h))\n",
    "    # JSON writer\n",
    "    json_data = []\n",
    "\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    ret = True\n",
    "    idx = 0\n",
    "    while ret:\n",
    "        ret, frame = cap.read()\n",
    "        if frame is None: break\n",
    "\n",
    "        x1, y1, x2, y2 = person[idx] # Normalized bounding box\n",
    "        if np.sum([x1, y1, x2, y2]) == 0:\n",
    "            print(f'No person in frame {idx}')\n",
    "            continue\n",
    "        # Denormalize\n",
    "        x1 = int(x1 * video_width)\n",
    "        x2 = int(x2 * video_width)\n",
    "        y1 = int(y1 * video_height)\n",
    "        y2 = int(y2 * video_height)\n",
    "        # Save the bounding box (video coords)\n",
    "        json_data.append({'frame': idx, 'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2})\n",
    "\n",
    "        # Get the center of the bounding box\n",
    "        center_x, center_y = ((x1 + x2) // 2, (y1 + y2) // 2)\n",
    "\n",
    "        # Get the new bounding box\n",
    "        x1_crop = max(0, center_x - bbox_max_w // 2)\n",
    "        x2_crop = min(frame.shape[1], center_x + bbox_max_w // 2)\n",
    "        y1_crop = max(0, center_y - bbox_max_h // 2)\n",
    "        y2_crop = min(frame.shape[0], center_y + bbox_max_h // 2)\n",
    " \n",
    "        # Correct for edge cases (pun intended)\n",
    "        if x1_crop == 0: x2_crop = bbox_max_w\n",
    "        if x2_crop == frame.shape[1]: x1_crop = frame.shape[1] - bbox_max_w\n",
    "        if y1_crop == 0: y2_crop = bbox_max_h\n",
    "        if y2_crop == frame.shape[0]: y1_crop = frame.shape[0] - bbox_max_h\n",
    "\n",
    "        cropped = frame[y1_crop:y2_crop, x1_crop:x2_crop]\n",
    "\n",
    "        cropped = cv2.resize(cropped, (bbox_max_w, bbox_max_h)) # Have to resize so video saves for some reason\n",
    "\n",
    "        out_cap.write(cropped)\n",
    "\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "    with open(out_json, 'w') as f:\n",
    "        json.dump(json_data, f)\n",
    "\n",
    "    cap.release()\n",
    "    out_cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f'Finished {v}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
