{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extrating frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import queue\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import imutils\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2 \n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import time\n",
    "from bisect import bisect_right\n",
    "from sktime.datatypes._panel._convert import from_2d_array_to_nested\n",
    "from court_detector import CourtDetector\n",
    "from TrackPlayers.trackplayers import *\n",
    "from utils import get_video_properties, get_dtype\n",
    "from detection import *  # provides diff_xy, remove_outliers, interpolation\n",
    "from pickle import load\n",
    "\n",
    "# -------------------------------\n",
    "# Parse command-line arguments\n",
    "# -------------------------------\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input_video_path\", default=\"/home/akash/ws/personal/sportsAI/src/dataset_prep/tennis-tracking/VideoInput/video_input2.mp4\", type=str)\n",
    "parser.add_argument(\"--output_video_path\", type=str, default=\"\")\n",
    "parser.add_argument(\"--minimap\", type=int, default=0)\n",
    "parser.add_argument(\"--bounce\", type=int, default=0)\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "input_video_path = args.input_video_path\n",
    "output_video_path = args.output_video_path\n",
    "minimap = args.minimap\n",
    "bounce = args.bounce\n",
    "\n",
    "# -------------------------------\n",
    "# File paths and YOLO parameters\n",
    "# -------------------------------\n",
    "yolo_classes = 'Yolov3/yolov3.txt'\n",
    "yolo_weights = 'Yolov3/yolov3.weights'\n",
    "yolo_config  = 'Yolov3/yolov3.cfg'\n",
    "CONF_THRESHOLD = 0.5\n",
    "\n",
    "if output_video_path == \"\":\n",
    "    output_video_path = input_video_path.split('.')[0] + \"VideoOutput/video_output.mp4\"\n",
    "\n",
    "# -------------------------------\n",
    "# Open input video and get properties\n",
    "# -------------------------------\n",
    "video = cv2.VideoCapture(input_video_path)\n",
    "if not video.isOpened():\n",
    "    raise FileNotFoundError(f\"Could not open video: {input_video_path}\")\n",
    "\n",
    "fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "output_width  = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "output_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "total = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# -------------------------------\n",
    "# Set up output video writer\n",
    "# -------------------------------\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_video = cv2.VideoWriter(output_video_path, fourcc, fps, (output_width, output_height))\n",
    "\n",
    "# -------------------------------\n",
    "# Load YOLO network and labels\n",
    "# -------------------------------\n",
    "with open(yolo_classes) as f:\n",
    "    LABELS = [line.strip() for line in f.readlines()]\n",
    "\n",
    "net = cv2.dnn.readNet(yolo_weights, yolo_config)\n",
    "layer_names = net.getLayerNames()\n",
    "print(net.getUnconnectedOutLayers())\n",
    "unconnected = net.getUnconnectedOutLayers()\n",
    "# Check the shape and index accordingly:\n",
    "if len(unconnected.shape) == 2:\n",
    "    output_layers = [layer_names[i[0] - 1] for i in unconnected]\n",
    "else:\n",
    "    output_layers = [layer_names[i - 1] for i in unconnected]\n",
    "\n",
    "# -------------------------------\n",
    "# Initialize court and player detectors\n",
    "# -------------------------------\n",
    "court_detector = CourtDetector()\n",
    "dtype = get_dtype()\n",
    "detection_model = DetectionModel(dtype=dtype)\n",
    "\n",
    "# Get video properties for later resizing\n",
    "fps, length, v_width, v_height = get_video_properties(video)\n",
    "\n",
    "# -------------------------------\n",
    "# Process video to detect court and players (first pass)\n",
    "# -------------------------------\n",
    "frames = []\n",
    "frame_i = 0\n",
    "print(\">>>>>>>>Extrating frames\")\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    frame_i += 1\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if frame_i == 1:\n",
    "        lines = court_detector.detect(frame)\n",
    "    else:\n",
    "        lines = court_detector.track_court(frame)\n",
    "    detection_model.detect_player_1(frame, court_detector)\n",
    "    detection_model.detect_top_persons(frame, court_detector, frame_i)\n",
    "    \n",
    "    # Draw detected court lines\n",
    "    for i in range(0, len(lines), 4):\n",
    "        x1, y1, x2, y2 = lines[i], lines[i+1], lines[i+2], lines[i+3]\n",
    "        cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255), 5)\n",
    "    \n",
    "    new_frame = cv2.resize(frame, (v_width, v_height))\n",
    "    frames.append(new_frame)\n",
    "    \n",
    "\n",
    "video.release()\n",
    "print(\">>>>>.frame extraction completed!!!\")\n",
    "detection_model.find_player_2_box()\n",
    "player1_boxes = detection_model.player_1_boxes\n",
    "player2_boxes = detection_model.player_2_boxes\n",
    "\n",
    "# -------------------------------\n",
    "# Second pass: Detect ball using YOLO (without TrackNet)\n",
    "# -------------------------------\n",
    "video = cv2.VideoCapture(input_video_path)\n",
    "currentFrame = 0\n",
    "coords = []  # Store ball center [x, y] per frame (or None if not detected)\n",
    "t = []       # Timestamps for each frame\n",
    "\n",
    "# Use a fixed-length deque to draw a short trail\n",
    "q = queue.deque([None]*8, maxlen=8)\n",
    "last_time = time.time()\n",
    "print(\">>>>>>>deetction started\")\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Prepare blob and run YOLO forward pass\n",
    "    blob = cv2.dnn.blobFromImage(frame, scalefactor=1/255.0, size=(416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward(output_layers)\n",
    "\n",
    "    ball_center = None\n",
    "    H, W = frame.shape[:2]\n",
    "    # Loop over each detection from each output layer\n",
    "    for output in detections:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "            # Check if the detected object is a sports ball and passes confidence threshold.\n",
    "            if LABELS[classID].lower() in [\"sports ball\", \"tennis ball\"] and confidence > CONF_THRESHOLD:\n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width_box, height_box) = box.astype(\"int\")\n",
    "                # Convert center coordinates from YOLO (center format) to top-left format if needed.\n",
    "                # Here we simply use the provided center coordinates.\n",
    "                ball_center = [centerX, centerY]\n",
    "                break\n",
    "        if ball_center is not None:\n",
    "            break\n",
    "\n",
    "    # Append detection result and update deque for drawing\n",
    "    coords.append(ball_center)\n",
    "    q.appendleft(ball_center)\n",
    "    t.append(time.time() - last_time)\n",
    "\n",
    "    # Optionally, mark player boxes\n",
    "    output_img = frame.copy()\n",
    "    output_img = mark_player_box(output_img, player1_boxes, currentFrame)\n",
    "    output_img = mark_player_box(output_img, player2_boxes, currentFrame)\n",
    "\n",
    "    # Draw ball detection trail on the frame\n",
    "    PIL_image = cv2.cvtColor(output_img, cv2.COLOR_BGR2RGB)\n",
    "    PIL_image = Image.fromarray(PIL_image)\n",
    "    for pt in q:\n",
    "        if pt is not None:\n",
    "            bbox = (pt[0] - 2, pt[1] - 2, pt[0] + 2, pt[1] + 2)\n",
    "            draw = ImageDraw.Draw(PIL_image)\n",
    "            draw.ellipse(bbox, outline='yellow')\n",
    "            del draw\n",
    "\n",
    "    opencvImage = cv2.cvtColor(np.array(PIL_image), cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(f\"./saved_frames/frame_{currentFrame:04d}.png\" , opencvImage)\n",
    "    output_video.write(opencvImage)\n",
    "    currentFrame += 1\n",
    "    print(f\"Detecting frame: {currentFrame}\")\n",
    "    \n",
    "\n",
    "video.release()\n",
    "output_video.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords2 = coords.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolating points to avoid occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Post-processing: Compute distance, velocity, and direction\n",
    "# -------------------------------\n",
    "# Remove outliers and interpolate missing ball positions (diff_xy, remove_outliers, interpolation from detection module)\n",
    "x_vals, y_vals = diff_xy(coords)\n",
    "remove_outliers(x_vals, y_vals, coords)\n",
    "coords = interpolation(coords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute step distances (Euclidean distance between consecutive frames)\n",
    "step_distances = []\n",
    "for i in range(len(coords) - 1):\n",
    "    if coords[i] is not None and coords[i+1] is not None:\n",
    "        dx = coords[i+1][0] - coords[i][0]\n",
    "        dy = coords[i+1][1] - coords[i][1]\n",
    "        step_distances.append(np.sqrt(dx**2 + dy**2))\n",
    "    else:\n",
    "        step_distances.append(0.0)\n",
    "\n",
    "# Compute cumulative distance (prefix sum)\n",
    "prefix_sum = [0.0]\n",
    "current_sum = 0.0\n",
    "for dist in step_distances:\n",
    "    current_sum += dist\n",
    "    prefix_sum.append(current_sum)\n",
    "\n",
    "# Compute velocity components per frame\n",
    "Vx, Vy, V = [], [], []\n",
    "for i in range(len(coords)-1):\n",
    "    if coords[i] is not None and coords[i+1] is not None:\n",
    "        delta_time = t[i+1] - t[i]\n",
    "        delta_time = delta_time if delta_time != 0 else 1e-5\n",
    "        vx = (coords[i+1][0] - coords[i][0]) / delta_time\n",
    "        vy = (coords[i+1][1] - coords[i][1]) / delta_time\n",
    "        Vx.append(vx)\n",
    "        Vy.append(vy)\n",
    "        V.append(np.sqrt(vx**2 + vy**2))\n",
    "    else:\n",
    "        Vx.append(0.0)\n",
    "        Vy.append(0.0)\n",
    "        V.append(0.0)\n",
    "\n",
    "# Pad velocities to match number of frames\n",
    "padded_Vx = [0.0] + Vx\n",
    "padded_Vy = [0.0] + Vy\n",
    "padded_V  = [0.0] + V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine direction of ball movement relative to players.\n",
    "directions = []\n",
    "for i in range(len(coords)):\n",
    "    if coords[i] is None:\n",
    "        directions.append(\"unknown\")\n",
    "        continue\n",
    "    ball_x, ball_y = coords[i]\n",
    "    vx = padded_Vx[i]\n",
    "    vy = padded_Vy[i]\n",
    "    # Get player boxes for the current frame (assumes these lists are as long as frame count)\n",
    "    p1_box = player1_boxes[i] if i < len(player1_boxes) else None\n",
    "    p2_box = player2_boxes[i] if i < len(player2_boxes) else None\n",
    "    if p1_box is None or p2_box is None:\n",
    "        directions.append(\"unknown\")\n",
    "        continue\n",
    "    p1_center = ((p1_box[0] + p1_box[2]) / 2, (p1_box[1] + p1_box[3]) / 2)\n",
    "    # print(p2_box)\n",
    "    try:\n",
    "        p2_center = ((p2_box[0] + p2_box[2]) / 2, (p2_box[1] + p2_box[3]) / 2)\n",
    "    except:\n",
    "        pass\n",
    "    d1 = np.sqrt((ball_x - p1_center[0])**2 + (ball_y - p1_center[1])**2)\n",
    "    d2 = np.sqrt((ball_x - p2_center[0])**2 + (ball_y - p2_center[1])**2)\n",
    "    closest_center = p1_center if d1 < d2 else p2_center\n",
    "    vector_to_player = (ball_x - closest_center[0], ball_y - closest_center[1])\n",
    "    velocity_vector = (vx, vy)\n",
    "    dot_product = vector_to_player[0]*velocity_vector[0] + vector_to_player[1]*velocity_vector[1]\n",
    "    directions.append('outgoing' if dot_product > 0 else 'incoming')\n",
    "\n",
    "# -------------------------------\n",
    "# Compute distance traveled since last hit using a heuristic based on player proximity\n",
    "# -------------------------------\n",
    "hits = []\n",
    "threshold = 500  # threshold distance for a hit event (adjust as needed)\n",
    "for i in range(1, len(directions)):\n",
    "    if directions[i-1] == 'incoming' and directions[i] == 'outgoing' and coords[i] is not None:\n",
    "        ball_x, ball_y = coords[i]\n",
    "        p1_box = player1_boxes[i] if i < len(player1_boxes) else None\n",
    "        p2_box = player2_boxes[i] if i < len(player2_boxes) else None\n",
    "        if p1_box is None or p2_box is None:\n",
    "            continue\n",
    "        p1_center = ((p1_box[0] + p1_box[2]) / 2, (p1_box[1] + p1_box[3]) / 2)\n",
    "        try:\n",
    "            p2_center = ((p2_box[0] + p2_box[2]) / 2, (p2_box[1] + p2_box[3]) / 2)\n",
    "        except:\n",
    "            pass\n",
    "        d1 = np.sqrt((ball_x - p1_center[0])**2 + (ball_y - p1_center[1])**2)\n",
    "        d2 = np.sqrt((ball_x - p2_center[0])**2 + (ball_y - p2_center[1])**2)\n",
    "        print(\">>>>>>\",d2)\n",
    "        if d1 < threshold or d2 < threshold:\n",
    "            hits.append({'frame': i, 'player': 1 if d1 < d2 else 2})\n",
    "\n",
    "hit_frames = sorted([hit['frame'] for hit in hits])\n",
    "distance_traveled = []\n",
    "for i in range(len(coords)):\n",
    "    idx = bisect_right(hit_frames, i) - 1\n",
    "    if idx >= 0:\n",
    "        last_hit = hit_frames[idx]\n",
    "        distance_traveled_i = prefix_sum[i] - prefix_sum[last_hit]\n",
    "    else:\n",
    "        distance_traveled_i = 0.0\n",
    "    distance_traveled.append(distance_traveled_i)\n",
    "\n",
    "# -------------------------------\n",
    "# Save results to CSV files\n",
    "# -------------------------------\n",
    "frame_data = []\n",
    "for i in range(len(coords)):\n",
    "    # In case of missing detection, fill with NaN or a placeholder.\n",
    "    ball_pos = coords[i] if coords[i] is not None else [np.nan, np.nan]\n",
    "    frame_info = {\n",
    "        'frame': i,\n",
    "        'x': ball_pos[0],\n",
    "        'y': ball_pos[1],\n",
    "        'vx': padded_Vx[i],\n",
    "        'vy': padded_Vy[i],\n",
    "        'speed': padded_V[i],\n",
    "        'direction': 0 if directions[i]==\"incoming\" else 1,\n",
    "        'distance_since_last_hit': distance_traveled[i]\n",
    "    }\n",
    "    frame_data.append(frame_info)\n",
    "\n",
    "hits_data = [{'frame': hit['frame'], 'player': hit['player']} for hit in hits]\n",
    "\n",
    "pd.DataFrame(frame_data).to_csv('ball_data.csv', index=False)\n",
    "pd.DataFrame(hits_data).to_csv('hits.csv', index=False)\n",
    "\n",
    "print(\"Processing complete. Ball data saved to 'ball_data.csv' and hit events to 'hits.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shot recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from argparse import ArgumentParser\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "\n",
    "sys.path.append(\"../tennis_shot_recognition\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "No GPU detected. Running on CPU.\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "print(tf.config.experimental.list_physical_devices(\"GPU\"))\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        print(\"Memory growth enabled for GPU:\", physical_devices[0])\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU detected. Running on CPU.\")\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices(\"GPU\")))\n",
    "\n",
    "from extract_human_pose import HumanPoseExtractor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from track_and_classify_with_rnn import GT, draw_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShotCounter:\n",
    "    \"\"\"Basic shot counter with a shot history\"\"\"\n",
    "\n",
    "    MIN_FRAMES_BETWEEN_SHOTS = 10\n",
    "\n",
    "    BAR_WIDTH = 30\n",
    "    BAR_HEIGHT = 170\n",
    "    MARGIN_ABOVE_BAR = 30\n",
    "    SPACE_BETWEEN_BARS = 55\n",
    "    TEXT_ORIGIN_X = 1075\n",
    "    BAR_ORIGIN_X = 1070\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nb_history = 10  # best history size IMO\n",
    "        self.probs = np.zeros((self.nb_history, 4))\n",
    "\n",
    "        self.nb_forehands = 0\n",
    "        self.nb_backhands = 0\n",
    "        self.nb_serves = 0\n",
    "\n",
    "        self.last_shot = \"neutral\"\n",
    "        self.frames_since_last_shot = self.MIN_FRAMES_BETWEEN_SHOTS\n",
    "\n",
    "        self.results = []\n",
    "\n",
    "    def update(self, probs, frame_id):\n",
    "        \"\"\"\n",
    "        Update current state with new shots probabilities\n",
    "        If one of the probability is over 50%, it can be considered as reliable\n",
    "        We need at least MIN_FRAMES_BETWEEN_SHOTS frames between two shots (backhand/forehand/serve)\n",
    "        Between each shot, we should normally go through a \"neutral state\" meaning that the player\n",
    "        is not currently hitting the ball\n",
    "        \"\"\"\n",
    "\n",
    "        self.probs[0 : self.nb_history - 1, :] = self.probs[1:, :].copy()\n",
    "        self.probs[-1, :] = probs\n",
    "\n",
    "        self.frames_since_last_shot += 1\n",
    "\n",
    "        means = np.mean(self.probs, axis=0)\n",
    "        if means[0] > 0.5:\n",
    "            # backhand currently\n",
    "            if (\n",
    "                self.last_shot == \"neutral\"\n",
    "                and self.frames_since_last_shot > self.MIN_FRAMES_BETWEEN_SHOTS\n",
    "            ):\n",
    "                self.nb_backhands += 1\n",
    "                self.last_shot = \"backhand\"\n",
    "                self.frames_since_last_shot = 0\n",
    "                self.results.append({\"FrameID\": frame_id, \"Shot\": self.last_shot})\n",
    "        elif means[1] > 0.5:\n",
    "            # forehand currently\n",
    "            if (\n",
    "                self.last_shot == \"neutral\"\n",
    "                and self.frames_since_last_shot > self.MIN_FRAMES_BETWEEN_SHOTS\n",
    "            ):\n",
    "                self.nb_forehands += 1\n",
    "                self.last_shot = \"forehand\"\n",
    "                self.frames_since_last_shot = 0\n",
    "                self.results.append({\"FrameID\": frame_id, \"Shot\": self.last_shot})\n",
    "        elif means[2] > 0.5:\n",
    "            # neutral currently\n",
    "            self.last_shot = \"neutral\"\n",
    "        elif means[3] > 0.5:\n",
    "            # serve currently\n",
    "            if (\n",
    "                self.last_shot == \"neutral\"\n",
    "                and self.frames_since_last_shot > self.MIN_FRAMES_BETWEEN_SHOTS\n",
    "            ):\n",
    "                self.nb_serves += 1\n",
    "                self.last_shot = \"serve\"\n",
    "                self.frames_since_last_shot = 0\n",
    "                self.results.append({\"FrameID\": frame_id, \"Shot\": self.last_shot})\n",
    "\n",
    "    def display(self, frame):\n",
    "        \"\"\"\n",
    "        Display shot count\n",
    "        Colorize last shot in green\n",
    "        \"\"\"\n",
    "\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            f\"Backhands = {self.nb_backhands}\",\n",
    "            (20, frame.shape[0] - 100),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            fontScale=1,\n",
    "            color=(0, 255, 0)\n",
    "            if (self.last_shot == \"backhand\" and self.frames_since_last_shot < 30)\n",
    "            else (0, 0, 255),\n",
    "            thickness=2,\n",
    "        )\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            f\"Forehands = {self.nb_forehands}\",\n",
    "            (20, frame.shape[0] - 60),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            fontScale=1,\n",
    "            color=(0, 255, 0)\n",
    "            if (self.last_shot == \"forehand\" and self.frames_since_last_shot < 30)\n",
    "            else (0, 0, 255),\n",
    "            thickness=2,\n",
    "        )\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            f\"Serves = {self.nb_serves}\",\n",
    "            (20, frame.shape[0] - 20),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            fontScale=1,\n",
    "            color=(0, 255, 0)\n",
    "            if (self.last_shot == \"serve\" and self.frames_since_last_shot < 30)\n",
    "            else (0, 0, 255),\n",
    "            thickness=2,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall_precision(gt, shots):\n",
    "    \"\"\"\n",
    "    Give some metrics to assess current performances, like\n",
    "    how many shots were missed (recall) or were false positives (precision)\n",
    "    \"\"\"\n",
    "    gt_numpy = gt.to_numpy()\n",
    "    nb_match = 0\n",
    "    nb_misses = 0\n",
    "    nb_fp = 0\n",
    "    fp_backhands = 0\n",
    "    fp_forehands = 0\n",
    "    fp_serves = 0\n",
    "    for gt_shot in gt_numpy:\n",
    "        found_match = False\n",
    "        for shot in shots:\n",
    "            if shot[\"Shot\"] == gt_shot[0]:\n",
    "                if abs(shot[\"FrameID\"] - gt_shot[1]) <= 30:\n",
    "                    found_match = True\n",
    "                    break\n",
    "        if found_match:\n",
    "            nb_match += 1\n",
    "        else:\n",
    "            nb_misses += 1\n",
    "\n",
    "    for shot in shots:\n",
    "        found_match = False\n",
    "        for gt_shot in gt_numpy:\n",
    "            if shot[\"Shot\"] == gt_shot[0]:\n",
    "                if abs(shot[\"FrameID\"] - gt_shot[1]) <= 30:\n",
    "                    found_match = True\n",
    "                    break\n",
    "        if not found_match:\n",
    "            nb_fp += 1\n",
    "            if shot[\"Shot\"] == \"backhand\":\n",
    "                fp_backhands += 1\n",
    "            elif shot[\"Shot\"] == \"forehand\":\n",
    "                fp_forehands += 1\n",
    "            elif shot[\"Shot\"] == \"serve\":\n",
    "                fp_serves += 1\n",
    "\n",
    "    precision = nb_match / (nb_match + nb_fp)\n",
    "    recall = nb_match / (nb_match + nb_misses)\n",
    "\n",
    "    print(f\"Recall {recall*100:.1f}%\")\n",
    "    print(f\"Precision {precision*100:.1f}%\")\n",
    "\n",
    "    print(\n",
    "        f\"FP: backhands = {fp_backhands}, forehands = {fp_forehands}, serves = {fp_serves}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget -q -O movenet.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/float16/4?lite-format=tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = ArgumentParser(\n",
    "        description=\"Track tennis player and display shot probabilities\"\n",
    "    )\n",
    "    parser.add_argument(\"video\", default=\"/home/akash/ws/personal/sportsAI/src/dataset_prep/tennis-tracking/VideoInput/video_input2.mp4\")\n",
    "    parser.add_argument(\"model\",default=\"/home/akash/ws/personal/sportsAI/src/dataset_prep/tennis_shot_recognition/tennis_fully_connected.h5\")\n",
    "    parser.add_argument(\"--evaluate\", help=\"Path to annotation file\")\n",
    "    parser.add_argument(\"-f\", type=int, help=\"Forward to\")\n",
    "    args = parser.parse_args([\"/home/akash/ws/personal/sportsAI/src/dataset_prep/tennis-tracking/VideoInput/video_input2.mp4\", \"/home/akash/ws/personal/sportsAI/src/dataset_prep/tennis_shot_recognition/tennis_fully_connected.h5\"])\n",
    "\n",
    "    shot_counter = ShotCounter()\n",
    "\n",
    "    if args.evaluate is not None:\n",
    "        gt = GT(args.evaluate)\n",
    "\n",
    "    m1 = keras.models.load_model(args.model)\n",
    "\n",
    "    cap = cv2.VideoCapture(args.video)\n",
    "\n",
    "    assert cap.isOpened()\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    human_pose_extractor = HumanPoseExtractor(frame.shape)\n",
    "\n",
    "    FRAME_ID = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        FRAME_ID += 1\n",
    "\n",
    "        if args.f is not None and FRAME_ID < args.f:\n",
    "            continue\n",
    "\n",
    "        assert frame is not None\n",
    "\n",
    "        human_pose_extractor.extract(frame)\n",
    "\n",
    "        # dont draw non-significant points/edges by setting probability to 0\n",
    "        human_pose_extractor.discard([\"left_eye\", \"right_eye\", \"left_ear\", \"right_ear\"])\n",
    "\n",
    "        features = human_pose_extractor.keypoints_with_scores.reshape(17, 3)\n",
    "        features = features[features[:, 2] > 0][:, 0:2].reshape(1, 13 * 2)\n",
    "\n",
    "        # start = time.time()\n",
    "        probs = (\n",
    "            m1.__call__(features)[0] if human_pose_extractor.roi.valid else np.zeros(4)\n",
    "        )\n",
    "        # end = time.time()\n",
    "        # print(\"predict from features\", end - start)\n",
    "\n",
    "        shot_counter.update(probs, FRAME_ID)\n",
    "\n",
    "        draw_probs(frame, np.mean(shot_counter.probs, axis=0))\n",
    "        shot_counter.display(frame)\n",
    "        # draw_probs(frame, [probs[0], probs[1], probs[2], 0])\n",
    "\n",
    "        if args.evaluate is not None:\n",
    "            gt.display(frame, FRAME_ID)\n",
    "\n",
    "        # Display results on original frame\n",
    "        human_pose_extractor.draw_results_frame(frame)\n",
    "        if (\n",
    "            shot_counter.frames_since_last_shot < 30\n",
    "            and shot_counter.last_shot != \"neutral\"\n",
    "        ):\n",
    "            human_pose_extractor.roi.draw_shot(frame, shot_counter.last_shot)\n",
    "\n",
    "        # Display results on original frame\n",
    "        human_pose_extractor.draw_results_frame(frame)\n",
    "        # cv2.imshow(\"Frame\", frame)\n",
    "        human_pose_extractor.roi.update(human_pose_extractor.keypoints_pixels_frame)\n",
    "\n",
    "        # cv2.imwrite(f\"videos/image_{FRAME_ID:05d}.png\", frame)\n",
    "\n",
    "        # k = cv2.waitKey(0)\n",
    "        # if k == 27:\n",
    "        #     break\n",
    "\n",
    "    # cap.release()\n",
    "    # cv2.destroyAllWindows()\n",
    "\n",
    "    print(shot_counter.results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'FrameID': 26, 'Shot': 'backhand'}, {'FrameID': 175, 'Shot': 'forehand'}, {'FrameID': 339, 'Shot': 'forehand'}, {'FrameID': 426, 'Shot': 'forehand'}, {'FrameID': 527, 'Shot': 'forehand'}, {'FrameID': 812, 'Shot': 'forehand'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(\n",
    "    video_path=\"/home/akash/ws/personal/sportsAI/src/dataset_prep/tennis-tracking/VideoInput/video_input2.mp4\",\n",
    "    model_path=\"/home/akash/ws/personal/sportsAI/src/dataset_prep/tennis_shot_recognition/tennis_fully_connected.h5\",\n",
    "    evaluate_path=None,\n",
    "    forward_frame=0\n",
    "):\n",
    "    \"\"\"\n",
    "    Process the video to track the tennis player and detect shots.\n",
    "    \n",
    "    Parameters:\n",
    "        video_path (str): Path to the input video.\n",
    "        model_path (str): Path to the trained model.\n",
    "        evaluate_path (str or None): Optional path to the annotation file.\n",
    "        forward_frame (int): Frame number to start processing from.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of tuples with (frame number, shot type) for detected shots.\n",
    "    \"\"\"\n",
    "    # Initialize the shot counter.\n",
    "    shot_counter = ShotCounter()\n",
    "\n",
    "    # Optional ground truth evaluation.\n",
    "    if evaluate_path is not None:\n",
    "        gt = GT(evaluate_path)\n",
    "\n",
    "    # Load the model.\n",
    "    m1 = keras.models.load_model(model_path)\n",
    "\n",
    "    # Open the video.\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(f\"Cannot open video: {video_path}\")\n",
    "\n",
    "    # Read the first frame to initialize the human pose extractor.\n",
    "    ret, frame = cap.read()\n",
    "    if not ret or frame is None:\n",
    "        raise RuntimeError(\"Unable to read the first frame of the video\")\n",
    "    human_pose_extractor = HumanPoseExtractor(frame.shape)\n",
    "\n",
    "    FRAME_ID = 0\n",
    "\n",
    "    # Process video frame by frame.\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        FRAME_ID += 1\n",
    "\n",
    "        # Skip frames if a starting frame is specified.\n",
    "        if FRAME_ID < forward_frame:\n",
    "            continue\n",
    "\n",
    "        # Extract pose and filter out less-significant keypoints.\n",
    "        human_pose_extractor.extract(frame)\n",
    "        human_pose_extractor.discard([\"left_eye\", \"right_eye\", \"left_ear\", \"right_ear\"])\n",
    "\n",
    "        # Prepare features (assuming 13 keypoints with x, y coordinates).\n",
    "        features = human_pose_extractor.keypoints_with_scores.reshape(17, 3)\n",
    "        # Select keypoints with non-zero probability and then reshape to a flat array.\n",
    "        features = features[features[:, 2] > 0][:, 0:2].reshape(1, 13 * 2)\n",
    "\n",
    "        # Get shot probabilities if the ROI is valid.\n",
    "        probs = (\n",
    "            m1(features)[0] if human_pose_extractor.roi.valid else np.zeros(4)\n",
    "        )\n",
    "\n",
    "        # Update shot counter with current probabilities.\n",
    "        shot_counter.update(probs, FRAME_ID)\n",
    "\n",
    "        # (Optional) Evaluate and display can be inserted here if needed.\n",
    "        # But for returning results, we omit drawing and displaying.\n",
    "\n",
    "        # Update ROI based on current frame keypoints.\n",
    "        human_pose_extractor.roi.update(human_pose_extractor.keypoints_pixels_frame)\n",
    "\n",
    "    cap.release()\n",
    "    \n",
    "    # shot_counter.results is assumed to be a list of (frame, shot type) tuples.\n",
    "    # You can modify this extraction logic if your ShotCounter stores results differently.\n",
    "    return shot_counter.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    results = process_video()\n",
    "    for frame_number, shot_type in results:\n",
    "        print(f\"Frame {frame_number}: {shot_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
